import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import scala.collection.mutable.ListBuffer
import org.educationaldatamining.tutorials.spark.VectorSum

// load the data
val kc = sqlContext.read.load("./data/kc_CTA1_01-4")
val tx = sqlContext.read.load("./data/tx_CTA1_01-4")

// get our first-attempts
val first_attempts = tx.filter( $"Attempt_At_Step" === 1 ).select( $"Transaction_Id", $"Level_Section".as("Section"), $"Anon_Student_Id".as("Student"), $"Time", $"Problem_Name".as("Problem"), $"Step_Name".as("Step"), $"Outcome", when( $"Outcome" === "OK", 1.0 ).otherwise(0.0).as("Result") ).orderBy( $"Student", $"Time" )

// find the # of practice attempts per KC (per student)
val practice_per_KC = first_attempts.join( kc, "Transaction_Id" ).select( $"Student", concat_ws( "_", $"Section", $"kc" ).as("Section-KC"), $"Transaction_Id", $"Time", $"Result", rank().over( Window.partitionBy("Student","Section", "kc").orderBy("Time") ).as( "Practice" )).cache

// let's have a look at our results:
practice_per_KC.show

// how many rows do we have?
practice_per_KC.count

// but how many are distinct transactions?
practice_per_KC.select($"Transaction_Id").distinct.count

// create KC features
val kcIndexer = new StringIndexer().setInputCol("Section-KC").setOutputCol("KC_Idx").fit(practice_per_KC)
val kcEncoder = new OneHotEncoder().setDropLast(false).setInputCol("KC_Idx").setOutputCol("KC_Feature")
// get a kc-feature per transaction
val kcenc = kcEncoder.transform( kcIndexer.transform(practice_per_KC) )

// have a look at our results:
kcenc.show

// we need a new feature which is the (# of practice opportunities) x (the kc feature vector)
val dot = udf{ ( a: Int, v: Vector ) => { val l = new ListBuffer[(Int,Double)]; v.foreachActive( (i,d) => l.append( ( i, a*d ) )  ); Vectors.sparse( v.size, l ) } }
val practice_feature = kcenc.withColumn( "Practice_Feature", dot( $"Practice", $"KC_Feature" ) )

// now we aggregate our KC feature vectors up to the transaction level
val kcVectorAdder = new VectorSum( kcIndexer.labels.length )
val features_per_tx = practice_feature.groupBy( $"Transaction_Id", $"Student", $"Result" ).agg( kcVectorAdder( $"KC_Feature" ).as("KC_Feature" ), kcVectorAdder( $"Practice_Feature" ).as("Practice_Feature") )

// what have we got now?
features_per_tx.show

// create a feature-vector per Student
val studentNameIndexer = new StringIndexer().setInputCol("Student").setOutputCol("Student_Idx")
val studentFeatureEncoder = new OneHotEncoder().setDropLast(false).setInputCol("Student_Idx").setOutputCol("Student_Feature")

// assemble a pipeline for the students
val studentPM = new Pipeline().setStages( Array( studentNameIndexer, studentFeatureEncoder ) ).fit(features_per_tx)

// generate our student features
val stuenc = studentPM.transform(features_per_tx)

// have a look at the results:
stuenc.show

// assemble the final feature vector from all others:
val va = new VectorAssembler().setInputCols( Array("Student_Feature","KC_Feature","Practice_Feature") ).setOutputCol("Features")
val afm_data = va.transform( stuenc ).select( $"Transaction_Id", $"Features", $"Result" ).cache

// let's set aside a 10% test set
val Array(training, test) = afm_data.randomSplit( Array( 0.9, 0.1 ) )

// train the logistic regression model!
val lr = new LogisticRegression().setMaxIter(10).setFitIntercept(false).setFeaturesCol("Features").setLabelCol("Result").setRawPredictionCol("LRraw").setProbabilityCol("LRprob").setPredictionCol("LRpred")

// train our model
val afm_model = lr.fit( training )

// apply the model to the test data:
val predictions = afm_model.transform(test)

// lets take a peak at these results:
predictions.show

// but how good is this model, really?
val bcEval = new BinaryClassificationEvaluator().setLabelCol("Result").setRawPredictionCol("LRraw")
print( bcEval.getMetricName +" for our AFM model is: "+ bcEval.evaluate(predictions) )
