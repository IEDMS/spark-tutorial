import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}
import org.apache.spark.ml.Pipeline
import org.educationaldatamining.tutorials.spark.VectorSum

// load the data
val kc = sqlContext.read.load("./data/kc_CTA1_01-4")
val tx = sqlContext.read.load("./data/tx_CTA1_01-4")

// transform our KCs
val kcIndexer = new StringIndexer().setInputCol("kc").setOutputCol("kc_Idx").fit(kc)
val kcEncoder = new OneHotEncoder().setDropLast(false).setInputCol("kc_Idx").setOutputCol("kc_Feature")
// get a kc-feature per transaction
val kcVectorAdder = new VectorSum( kcIndexer.labels.length )
val kcenc = kcEncoder.transform( kcIndexer.transform(kc) )
val kcFeaturesByTx = kcenc.groupBy( $"Transaction_Id" ).agg( kcVectorAdder( $"kc_Feature" ) )

// create some transformers to encode our data
val studentNameIndexer = new StringIndexer().setInputCol("Anon_Student_Id").setOutputCol("Student_Idx").fit(tx)
val studentFeatureEncoder = new OneHotEncoder().setDropLast(false).setInputCol("Student_Idx").setOutputCol("Student_Feature")

// assemble everything into a pipeline
val pipeline = new Pipeline().setStages( Array( studentNameIndexer, studentFeatureEncoder ) )